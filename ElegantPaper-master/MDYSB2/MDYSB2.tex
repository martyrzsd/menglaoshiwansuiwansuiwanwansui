%!TEX program = xelatex
% 完整编译方法 1 pdflatex -> bibtex -> pdflatex -> pdflatex
% 完整编译方法 2: xelatex -> bibtex -> xelatex -> xelatex
\documentclass[lang=cn,11pt]{elegantpaper}

\title{写他妈的}




% 不需要版本信息，直接注释即可
\version{0.07}
% 不需要时间信息的话，需要把 \today 删除。
\date{}


% 如果想修改参考文献样式，请把这行注释掉
% \usepackage[authoryear]{gbt7714}  % 国标

\begin{document}

\maketitle


\section{模型灵感}

近年来随着机器学习在各种各样应用领域的成功, 越来越多的领域开始引入机器学习作为工具. 偏微分方程的研究也是其中之一. 偏微分方程判定主要需要解决这样一个问题, 



考虑一个一般的非线性偏微分方程

\begin{equation}
	u_t=\mathcal N(t,x,u_x,u_{xx},...),
\end{equation}

其中$\mathcal N$是一个非线性函数, 角标代表偏微分的方向. 那么我们给定了一系列数据$w_t(x,t)$, 其中$x$为空间中的网格点,$0\leq t\leq M$为时间戳, 我们能否根据空间偏导数的数据来判定这个偏微分方程的形式? 在最近的一些研究中, 有学者采用了神经网络(深度网络)来在这方面有了一些进展.

如果我们能够有一些对于这个方程的先验知识, 比如这个方程大概是一个二次偏微分方程等等, 那么我们便可以通过字典学习的方式来完成这样一个任务.

对于一个具有二次型非线性项的二阶偏微分方程, 我们可以给出它的一般形式.

\begin{equation}
	u_t=\alpha_1+\alpha_2u+\alpha_3u^2+\alpha_4u_x+\alpha_5u_x^2+\alpha_6uu_x+\alpha_7u_{xx}+\alpha_8u_{xx}^2+\alpha_9uu_{xx}+\alpha_{10}u_xu_{xx}
\end{equation}

那么这样的方程的阶数的确定可以根据经验及其物理意义或者是通过数值方法. 那么上面的方程还可以写成

\begin{equation}
	u_t=[1\quad u\quad u^2\quad u_x\quad u_x^2\quad uu_x\quad u_{xx}\quad u_{xx}^2\quad uu_{xx}\quad u_xu_{xx}] \mathbf \alpha
\end{equation}

其中我们可以把它归结为一项是字典项$F_u(t)$, 一项是系数项$\alpha$. 那么这个时候我们可以通过去优化

\begin{equation}
	\min_\alpha \dfrac{1}{2}\sum_{k=1}^M ||u_t(t_k)-F_U(t_k)\alpha||_2^2+\lambda||\alpha||_1,
\end{equation}

来获得这样的向量$\alpha$, 从而对于由字典里的函数通过线性组合能够表达出来的函数获得较好的结果. 其中$L_1$正则项的假设是比较自然的, $L_1$范数惩罚往往能够使得向量的坐标趋于稀疏, 因为其导数的线性性质. 在方程阶数增加时, 字典的大小是指数级增长的, 不过偏微分方程一般无法张满整个字典, 往往都是其中的几项起着作用. 所以稀疏的表示往往更符合这样具有物理含义的偏微分方程的规律, 这也符合奥卡姆的剃刀原理.

另外一种思路便是不加上这样偏微分方程非线项是二次型的假设, 直接使用神经网络来去近似这样的非线性性, 也就是说我们需要习得一个网络$\mathcal N$使得$u_t=\mathcal N(u,u_x,u_{xx})$(更高阶的字典也可以实现). 通过选择激活函数跟调整网络超参数来得到一个比较好的近似. 我们的方法的灵感也是来源于这里. 虽不过这种方法在失去了假设的情况下缺少可解释性. 我们并不知道这样的神经网络到底能不能或者在什么程度上表示什么形式的多么复杂的偏微分方程. 并且这样用神经网络复原出来的方程在长时间的演变中能否真的近似院方称?


\nocite{*}

% 如果想修改参考文献样式（非国标），请把下行取消注释，并换成合适的样式（比如 unsrt，plain 样式）。
\bibliographystyle{unsrt}
\bibliography{wpref}

\end{document}
