
@article{artic,
   author="et al., Silver D. Huang A. Maddison C.", 
   title="Mastering the game of Go with deep neural networks and tree search",
   journal={Nature},
   volume={529},
   pages={484--489},
   year={2016},
   publisher={Emerald Publishing Limited}
}

@misc{GT,
   author = {GoogleTrend},
   title = {机器学习 - 探索 - Google 趋势},
   howpublished = {\url{https://trends.google.com/trends/explore?date=all&q=%2Fm%2F01hyh_}}
}

@misc{AS1,
   author = {EncyclopædiaBritannica},
   title = {Arthur Samuel | American computer scientist | Britannica},
   howpublished = {\url{https://www.britannica.com/biography/Arthur-Samuel}}
}

@misc{AS2,
   author = {IEEEComputerSociety},
   title = {Arthur Samuel  | IEEE Computer Society},
   howpublished = {\url{https://www.computer.org/profiles/arthur-samuel}}
}

@misc{Youtube,
   author = {Geoffrey Hinton and Yann LeCun},
   title = {Turing Award Lecture "The Deep Learning Revolution"},
   howpublished={\url{https://www.youtube.com/watch?v=VsnQf7exv5I}}
}

@misc{Dartmouth,
   author = {John McCarthy},
   title = {A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE},
   howpublished = {\url{https://www.computer.org/profiles/arthur-samuel}}
}

@Inbook{Turing2009,
author="Turing, Alan M.",
editor="Epstein, Robert
and Roberts, Gary
and Beber, Grace",
title="Computing Machinery and Intelligence",
bookTitle="Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer",
year="2009",
publisher="Springer Netherlands",
address="Dordrecht",
pages="23--65",
abstract="I propose to consider the question, ``Can machines think?''♣ This should begin with definitions of the meaning of the terms ``machine'' and ``think''. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words ``machine'' and ``think'' are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ``Can machines think?'' is to be sought in a statistical survey such as a Gallup poll.",
isbn="978-1-4020-6710-5",
doi="10.1007/978-1-4020-6710-5_3",
url="https://doi.org/10.1007/978-1-4020-6710-5_3"
}

@book{tsybakov2008introduction,
  title={Introduction to nonparametric estimation},
  author={Tsybakov, Alexandre B},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{Candes2009,
abstract = {We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen? We show that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries. We prove that if the number m of sampled entries obeys m ≥ C n1.2r log n for some positive numerical constant C, then with very high probability, most n×n matrices of rank r can be perfectly recovered by solving a simple convex optimization program. This program finds the matrix with minimum nuclear norm that fits the data. The condition above assumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar results hold for arbitrary rectangular matrices as well. Our results are connected with the recent literature on compressed sensing, and show that objects other than signals and images can be perfectly reconstructed from very limited information. {\textcopyright} The Author(s) 2009.},
author = {Cand{\`{e}}s, Emmanuel J. and Recht, Benjamin},
doi = {10.1007/s10208-009-9045-5},
issn = {16153375},
journal = {Foundations of Computational Mathematics},
keywords = {Compressed sensing,Convex optimization,Decoupling,Duality in optimization,Low-rank matrices,Matrix completion,Noncommutative Khintchine inequality,Nuclear norm minimization,Random matrices},
title = {{Exact matrix completion via convex optimization}},
year = {2009}
}

@article{Recht2011,
abstract = {This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Cand{\`{e}}s and Recht (2009), Cand{\`{e}}s and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory. {\textcopyright} 2011 Benjamin Recht.},
archivePrefix = {arXiv},
arxivId = {0910.0651},
author = {Recht, Benjamin},
eprint = {0910.0651},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Compressed sensing,Convex optimization,Low-rank matrices,Matrix completion,Nuclear norm minimization,Operator chernoff bound,Random matrices},
title = {{A simpler approach to matrix completion}},
year = {2011}
}

@inproceedings{Eriksson2012,
abstract = {This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank. Since the columns belong to a union of subspaces, this problem may also be viewed as a missing-data version of the subspace clustering problem. Let Xbe an n×N matrix whose (complete) columns lie in a union of at most k subspaces, each of rank ≤ r {\textless} n, and assume N ≥ kn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least CrN log2(n) entries of Xare observed uniformly at random, with C {\textgreater} 1 a constant depending on the usual incoherence conditions, the geometrical arrangement of subspaces, and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.},
author = {Eriksson, Brian and Balzano, Laura and Nowak, Robert},
booktitle = {Journal of Machine Learning Research},
issn = {15337928},
title = {{High-rank matrix completion}},
year = {2012}
}

@article{Candes2010,
abstract = {This paper is concerned with the problem of recovering an unknown matrix from a small fraction of its entries. This is known as the matrix completion problem, and comes up in a great number of applications, including the famous Netflix Prize and other similar questions in collaborative filtering. In general, accurate recovery of a matrix from a small number of entries is impossible, but the knowledge that the unknown matrix has low rank radically changes this premise, making the search for solutions meaningful. This paper presents optimality results quantifying the minimum number of entries needed to recover a matrix of rank r exactly by any method whatsoever (the information theoretic limit). More importantly, the paper shows that, under certain incoherence assumptions on the singular vectors of the matrix, recovery is possible by solving a convenient convex program as soon as the number of entries is on the order of the information theoretic limit (up to logarithmic factors). This convex program simply finds, among all matrices consistent with the observed entries, that with minimum nuclear norm. As an example, we show that on the order of nrlog(n) samples are needed to recover a random n × n matrix of rank r by any method, and to be sure, nuclear norm minimization succeeds as soon as the number of entries is of the form nrpolylog(n). {\textcopyright} 2010 IEEE.},
archivePrefix = {arXiv},
arxivId = {0903.1476},
author = {Cand{\`{e}}s, Emmanuel J. and Tao, Terence},
doi = {10.1109/TIT.2010.2044061},
eprint = {0903.1476},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Duality in optimization,Free probability,Low-rank matrices,Matrix completion,Nuclear norm minimization,Random matrices and techniques from random matrix theory,Semidefinite programming},
title = {{The power of convex relaxation: Near-optimal matrix completion}},
year = {2010}
}

@article{Yarotsky2017,
abstract = {We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.},
archivePrefix = {arXiv},
arxivId = {1610.01145},
author = {Yarotsky, Dmitry},
doi = {10.1016/j.neunet.2017.07.002},
eprint = {1610.01145},
issn = {18792782},
journal = {Neural Networks},
keywords = {Approximation complexity,Deep ReLU networks},
title = {{Error bounds for approximations with deep ReLU networks}},
year = {2017}
}

@article{Vapnik1994,
abstract = {A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.},
author = {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
doi = {10.1162/neco.1994.6.5.851},
issn = {0899-7667},
journal = {Neural Computation},
title = {{Measuring the VC-Dimension of a Learning Machine}},
year = {1994}
}

@article{Goldberg1992,
author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
doi = {10.1145/138859.138867},
issn = {15577317},
journal = {Communications of the ACM},
keywords = {information filtering,tapestry},
title = {{Using collaborative filtering to Weave an Information tapestry}},
year = {1992}
}

@inproceedings{kosiorek2019stacked,
  title={Stacked capsule autoencoders},
  author={Kosiorek, Adam and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15486--15496},
  year={2019}
}

@misc{hessel2017rainbow,
    title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
    author={Matteo Hessel and Joseph Modayil and Hado van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Azar and David Silver},
    year={2017},
    eprint={1710.02298},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@book{机器学习,
  title={机器学习},
  author={周志华},
  year={2016},
  publisher={清华大学出版社}
}

@book{DeepLearning,
  title={Deep Learning},
  author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  year={2016},
  publisher={The MIT Press}
}

@article{Raissi2018,
abstract = {We put forth a deep learning approach for discovering nonlinear partial differential equations from scattered and potentially noisy observations in space and time. Specifically, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. the first network acts as a prior on the unknown solution and essentially enables us to avoid numerical differentiations which are inherently ill-conditioned and unstable. the second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the effectiveness of our approach for several benchmark problems spanning a number of scientific domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers', Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr{\"{o}}dinger, and Navier-Stokes equations.},
archivePrefix = {arXiv},
arxivId = {1801.06637},
author = {Raissi, Maziar},
eprint = {1801.06637},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Big Data,Data-driven Scientific Discovery,Nonlinear Dynamics,Physics Informed Machine Learning,Predictive Modeling,Systems Identification},
mendeley-groups = {Previous/Neural network with LIAO,MDYNMSL},
pages = {1--24},
title = {{Deep hidden physics models: Deep learning of nonlinear partial differential equations}},
volume = {19},
year = {2018}
}
@article{Schaeffer2017,
abstract = {We investigate the problem of learning an evolution equation directly from some given data. This work develops a learning algorithm to identify the terms in the underlying partial differential equations and to approximate the coefficients of the terms only using data. The algorithm uses sparse optimization in order to perform feature selection and parameter estimation. The features are data driven in the sense that they are constructed using nonlinear algebraic equations on the spatial derivatives of the data. Several numerical experiments show the proposed method's robustness to data noise and size, its ability to capture the true features of the data, and its capability of performing additional analytics. Examples include shock equations, pattern formation, fluid flow and turbulence, and oscillatory convection.},
author = {Schaeffer, Hayden},
doi = {10.1098/rspa.2016.0446},
issn = {14712946},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Feature selection,Machine learning,Parameter estimation,Partial differential equations,Sparse optimization},
mendeley-groups = {Previous/Neural network with LIAO,MDYNMSL},
number = {2197},
title = {{Learning partial differential equations via data discovery and sparse optimization}},
volume = {473},
year = {2017}
}
@article{long2017pde,
  title={PDE-net: Learning PDEs from data},
  author={Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
  journal={arXiv preprint arXiv:1710.09668},
  year={2017}
}
@article{long2019pde,
  title={PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network},
  author={Long, Zichao and Lu, Yiping and Dong, Bin},
  journal={Journal of Computational Physics},
  volume={399},
  pages={108925},
  year={2019},
  publisher={Elsevier}
}