
@article{artic,
   author="et al., Silver D. Huang A. Maddison C.", 
   title="Mastering the game of Go with deep neural networks and tree search",
   journal={Nature},
   volume={529},
   pages={484--489},
   year={2016},
   publisher={Emerald Publishing Limited}
}

@misc{GT,
   author = {GoogleTrend},
   title = {机器学习 - 探索 - Google 趋势},
   howpublished = {\url{https://trends.google.com/trends/explore?date=all&q=%2Fm%2F01hyh_}}
}

@misc{AS1,
   author = {EncyclopædiaBritannica},
   title = {Arthur Samuel | American computer scientist | Britannica},
   howpublished = {\url{https://www.britannica.com/biography/Arthur-Samuel}}
}

@misc{AS2,
   author = {IEEEComputerSociety},
   title = {Arthur Samuel  | IEEE Computer Society},
   howpublished = {\url{https://www.computer.org/profiles/arthur-samuel}}
}


@misc{Dartmouth,
   author = {John McCarthy},
   title = {A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE},
   howpublished = {\url{https://www.computer.org/profiles/arthur-samuel}}
}

@misc{net,
   author = {@hanbingtao},
   title = {零基础入门深度学习(3) - 神经网络和反向传播算法},
   howpublished = {\url{https://www.zybuluo.com/hanbingtao/note/476663}}
}

@Inbook{Turing2009,
author="Turing, Alan M.",
editor="Epstein, Robert
and Roberts, Gary
and Beber, Grace",
title="Computing Machinery and Intelligence",
bookTitle="Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer",
year="2009",
publisher="Springer Netherlands",
address="Dordrecht",
pages="23--65",
abstract="I propose to consider the question, ``Can machines think?''♣ This should begin with definitions of the meaning of the terms ``machine'' and ``think''. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words ``machine'' and ``think'' are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ``Can machines think?'' is to be sought in a statistical survey such as a Gallup poll.",
isbn="978-1-4020-6710-5",
doi="10.1007/978-1-4020-6710-5_3",
url="https://doi.org/10.1007/978-1-4020-6710-5_3"
}

@book{tsybakov2008introduction,
  title={Introduction to nonparametric estimation},
  author={Tsybakov, Alexandre B},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{Candes2009,
abstract = {We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen? We show that one can perfectly recover most low-rank matrices from what appears to be an incomplete set of entries. We prove that if the number m of sampled entries obeys m ≥ C n1.2r log n for some positive numerical constant C, then with very high probability, most n×n matrices of rank r can be perfectly recovered by solving a simple convex optimization program. This program finds the matrix with minimum nuclear norm that fits the data. The condition above assumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25, then the result holds for all values of the rank. Similar results hold for arbitrary rectangular matrices as well. Our results are connected with the recent literature on compressed sensing, and show that objects other than signals and images can be perfectly reconstructed from very limited information. {\textcopyright} The Author(s) 2009.},
author = {Cand{\`{e}}s, Emmanuel J. and Recht, Benjamin},
doi = {10.1007/s10208-009-9045-5},
issn = {16153375},
journal = {Foundations of Computational Mathematics},
keywords = {Compressed sensing,Convex optimization,Decoupling,Duality in optimization,Low-rank matrices,Matrix completion,Noncommutative Khintchine inequality,Nuclear norm minimization,Random matrices},
title = {{Exact matrix completion via convex optimization}},
year = {2009}
}

@article{Recht2011,
abstract = {This paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct an unknown low-rank matrix. These results improve on prior work by Cand{\`{e}}s and Recht (2009), Cand{\`{e}}s and Tao (2009), and Keshavan et al. (2009). The reconstruction is accomplished by minimizing the nuclear norm, or sum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying matrix satisfies a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic factor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self contained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum information theory. {\textcopyright} 2011 Benjamin Recht.},
archivePrefix = {arXiv},
arxivId = {0910.0651},
author = {Recht, Benjamin},
eprint = {0910.0651},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Compressed sensing,Convex optimization,Low-rank matrices,Matrix completion,Nuclear norm minimization,Operator chernoff bound,Random matrices},
title = {{A simpler approach to matrix completion}},
year = {2011}
}

@inproceedings{Eriksson2012,
abstract = {This paper considers the problem of completing a matrix with many missing entries under the assumption that the columns of the matrix belong to a union of multiple low-rank subspaces. This generalizes the standard low-rank matrix completion problem to situations in which the matrix rank can be quite high or even full rank. Since the columns belong to a union of subspaces, this problem may also be viewed as a missing-data version of the subspace clustering problem. Let Xbe an n×N matrix whose (complete) columns lie in a union of at most k subspaces, each of rank ≤ r {\textless} n, and assume N ≥ kn. The main result of the paper shows that under mild assumptions each column of X can be perfectly recovered with high probability from an incomplete version so long as at least CrN log2(n) entries of Xare observed uniformly at random, with C {\textgreater} 1 a constant depending on the usual incoherence conditions, the geometrical arrangement of subspaces, and the distribution of columns over the subspaces. The result is illustrated with numerical experiments and an application to Internet distance matrix completion and topology identification.},
author = {Eriksson, Brian and Balzano, Laura and Nowak, Robert},
booktitle = {Journal of Machine Learning Research},
issn = {15337928},
title = {{High-rank matrix completion}},
year = {2012}
}

@article{Candes2010,
abstract = {This paper is concerned with the problem of recovering an unknown matrix from a small fraction of its entries. This is known as the matrix completion problem, and comes up in a great number of applications, including the famous Netflix Prize and other similar questions in collaborative filtering. In general, accurate recovery of a matrix from a small number of entries is impossible, but the knowledge that the unknown matrix has low rank radically changes this premise, making the search for solutions meaningful. This paper presents optimality results quantifying the minimum number of entries needed to recover a matrix of rank r exactly by any method whatsoever (the information theoretic limit). More importantly, the paper shows that, under certain incoherence assumptions on the singular vectors of the matrix, recovery is possible by solving a convenient convex program as soon as the number of entries is on the order of the information theoretic limit (up to logarithmic factors). This convex program simply finds, among all matrices consistent with the observed entries, that with minimum nuclear norm. As an example, we show that on the order of nrlog(n) samples are needed to recover a random n × n matrix of rank r by any method, and to be sure, nuclear norm minimization succeeds as soon as the number of entries is of the form nrpolylog(n). {\textcopyright} 2010 IEEE.},
archivePrefix = {arXiv},
arxivId = {0903.1476},
author = {Cand{\`{e}}s, Emmanuel J. and Tao, Terence},
doi = {10.1109/TIT.2010.2044061},
eprint = {0903.1476},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Duality in optimization,Free probability,Low-rank matrices,Matrix completion,Nuclear norm minimization,Random matrices and techniques from random matrix theory,Semidefinite programming},
title = {{The power of convex relaxation: Near-optimal matrix completion}},
year = {2010}
}

@article{Yarotsky2017,
abstract = {We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.},
archivePrefix = {arXiv},
arxivId = {1610.01145},
author = {Yarotsky, Dmitry},
doi = {10.1016/j.neunet.2017.07.002},
eprint = {1610.01145},
issn = {18792782},
journal = {Neural Networks},
keywords = {Approximation complexity,Deep ReLU networks},
title = {{Error bounds for approximations with deep ReLU networks}},
year = {2017}
}

@article{Vapnik1994,
abstract = {A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.},
author = {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
doi = {10.1162/neco.1994.6.5.851},
issn = {0899-7667},
journal = {Neural Computation},
title = {{Measuring the VC-Dimension of a Learning Machine}},
year = {1994}
}

@article{Goldberg1992,
author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
doi = {10.1145/138859.138867},
issn = {15577317},
journal = {Communications of the ACM},
keywords = {information filtering,tapestry},
title = {{Using collaborative filtering to Weave an Information tapestry}},
year = {1992}
}


